hardware:
    use_gpu: True
    which_gpu: 0
    dataset_folder_path: '/images/rat'  # Needs a "/" at the beginning

env:
    img_size: 64

# Visual AE parameters
vae:
    do_train: True
    train_batch_size: 50
    eval_batch_size: 50
    n_epochs: 1
    n_iter: 1
    in_channels: 3
    latent_dim: 64  # Dimension of the output vector of the encoder
    output_channels: [16, 16, 32, 32]
    kernel_sizes: [5, 5, 3, 3]
    strides: [2, 2, 1, 1]
    paddings: [2, 2, 1, 1] # Same padding
    learning_rate: 1e-4
    net_config:
      {
        "output_channels": [16, 16, 32, 32],
        "kernel_sizes": [5, 5, 3, 3],
        "strides": [2, 2, 1, 1],
        "paddings": [2, 2, 1, 1],
        "output_paddings": [0, 0, 1, 1]  # for deconvolution
      }
    save_top_k: 3  # Save the top 3 models from the training processq

# Spatial Memory Pipeline (SMP) parameters
smp:
    n_iter: 200000
    entropy_reactivation_target: 0.5  # H_react
    prob_correction: 0.1  # P_correction
    prob_storage: 0.0000625  # P_storage
    nb_memory_slots: 512  # S
    episode_len: 300
    unroll_len: 50
    batch_size: 50
    memory_slot_learning_rate: 0.01
    learning_rate: 0.003
    ae_learning_rate: 0.0001
    rnn_dropout: 0.5
    adam_beta1: 0.9  # beta_1
    adam_beta2: 0.999  # beta_2

rl:
    bptt_unroll_length: 100
    batch_size: 64
    actor_critic_lr: 1e-3  # (actor-critic loss), 5e-5 for top-down schematic representation with RL-baseline
    base_loss_coeff: 0.5
    entropy_loss_coeff: 1e-3
    rnn_lr: 1e-3
    memory_embed_lr: 3e-3
    rnn_dropout: 0.5

logging:
    seed: 1234
    exp: test_traj
    n_traj: 2