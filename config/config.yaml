experiment_type: "rl"  # ["vae", "smp", "rl"]

hardware:
    accelerator: 'gpu' # ['cpu', 'gpu', 'tpu']
    which_gpu: 0
    matmul_precision: "high"
    smp_dataset_folder_path: '/data/'
    num_data_loader_workers: 16  # Set to you numbers of CPU cores for smp

env:
    img_size: 64

# Visual AE parameters
vae:
    do_train: True
    train_batch_size: 50
    eval_batch_size: 50
    max_epochs: 625
    max_steps: 200000
    in_channels: 3
    latent_dim: 64  # Dimension of the output vector of the encoder
    learning_rate: 1e-4
    net_config:
      {
        "output_channels": [16, 16, 32, 32],
        "kernel_sizes": [5, 5, 3, 3],
        "strides": [2, 2, 1, 1],
        "paddings": [2, 2, 1, 1],
        "output_paddings": [0, 0, 1, 1]  # for deconvolution
      }
    save_top_k: 3  # Save the top 3 models from the training process

# Spatial Memory Pipeline (SMP) parameters
smp:
    n_iter: 2 #200000
    max_epochs: 800
    max_steps: 200000
    entropy_reactivation_target: 0.5  # H_react
    beta: 2.0  # beta
    prob_correction: 0.1  # P_correction
    prob_storage: 0.0000625  # P_storage
    nb_memory_slots: 512  # S
    episode_len: 25
    bptt_unroll_length: 50
    batch_size: 32
    memory_slot_learning_rate: 0.01
    learning_rate: 0.003
    rnn_dropout: 0.5
    adam_beta1: 0.9  # beta_1
    adam_beta2: 0.999  # beta_2
    ae_checkpoint_path: "/checkpoints/rat_ae-epoch=23-train_loss=0.001255.ckpt"
    saved_model_path: False # [False, "/checkpoints/epoch=189-step=47500.ckpt"]
    hidden_size_RNN1: 32
    hidden_size_RNN2: 128
    hidden_size_RNN3: 128

rl:
    bptt_unroll_length: 50
    batch_size: 64
    actor_critic_lr: 1e-3  # (actor-critic loss), 5e-5 for top-down schematic representation with RL-baseline
    base_loss_coeff: 0.5
    entropy_loss_coeff: 1e-3
    rnn_lr: 1e-3
    memory_embed_lr: 3e-3
    rnn_dropout: 0.5

rlsmp:
    n_iter: 2 #200000
    max_epochs: 10
    max_steps: 200000
    entropy_reactivation_target: 1.0  # H_react
    beta: 2.0  # beta
    prob_correction: 0.1  # P_correction
    prob_storage: 0.0001  # P_storage
    nb_memory_slots: 1024  # S
    episode_len: 25
    bptt_unroll_length: 20
    batch_size: 3
    memory_slot_learning_rate: 0.003
    learning_rate: 0.001
    rnn_dropout: 0.5
    adam_beta1: 0.9  # beta_1
    adam_beta2: 0.999  # beta_2
    ae_checkpoint_path: "/checkpoints/rat_ae-epoch=23-train_loss=0.001255.ckpt"
    hidden_size_RNN1: 32
    hidden_size_RNN2: 768
    hidden_size_RNN3: 128
    policy_n_layers: 1
    policy_net_size: 256
    sac_grad_norm_clipping: 10
    target_update_freq: 1
    polyak_avg: 0.99
    sac_gamma: 0.99
    sac_entropy_coeff: 0.2
    replay_buffer_size: 1000000

logging:
    seed: 1234
    exp: traj
    n_traj: 800
