experiment_type: "rl"  # ["vae", "smp", "rl"]

hardware:
    use_gpu: True
    which_gpu: 0
    matmul_precision: "high"
    dataset_folder_path: '/images/rat'  # Needs a "/" at the beginning
    smp_dataset_folder_path: '/data'
    num_data_loader_workers: 8  # Set to you numbers of CPU cores for smp

env:
    img_size: 64

# Visual AE parameters
vae:
    do_train: True
    train_batch_size: 50
    eval_batch_size: 50
    max_epochs: 420
    max_steps: 200000
    in_channels: 3
    latent_dim: 64  # Dimension of the output vector of the encoder
    learning_rate: 1e-4
    net_config:
      {
        "output_channels": [16, 16, 32, 32],
        "kernel_sizes": [5, 5, 3, 3],
        "strides": [2, 2, 1, 1],
        "paddings": [2, 2, 1, 1],
        "output_paddings": [0, 0, 1, 1]  # for deconvolution
      }
    save_top_k: 3  # Save the top 3 models from the training process

# Spatial Memory Pipeline (SMP) parameters
smp:
    n_iter: 2 #200000
    max_epochs: 2
    max_steps: 200000
    entropy_reactivation_target: 0.5  # H_react
    beta: 1.0  # beta
    prob_correction: 0.1  # P_correction
    prob_storage: 0.0000625  # P_storage
    nb_memory_slots: 512  # S
    episode_len: 25
    bptt_unroll_length: 50
    batch_size: 32
    memory_slot_learning_rate: 0.01
    learning_rate: 0.003
    rnn_dropout: 0.5
    adam_beta1: 0.9  # beta_1
    adam_beta2: 0.999  # beta_2
    ae_checkpoint_path: "/checkpoints/rat_autoencoder-epoch=402-train_loss=0.00.ckpt"
    hidden_size_RNN1: 32
    hidden_size_RNN2: 128
    hidden_size_RNN3: 128

rl:
    bptt_unroll_length: 50
    batch_size: 64
    actor_critic_lr: 1e-3  # (actor-critic loss), 5e-5 for top-down schematic representation with RL-baseline
    base_loss_coeff: 0.5
    entropy_loss_coeff: 1e-3
    rnn_lr: 1e-3
    memory_embed_lr: 3e-3
    rnn_dropout: 0.5

rlsmp:
    n_iter: 2 #200000
    max_epochs: 10
    max_steps: 200000
    entropy_reactivation_target: 1.0  # H_react
    prob_correction: 0.1  # P_correction
    prob_storage: 0.0001  # P_storage
    nb_memory_slots: 1024  # S
    episode_len: 25
    bptt_unroll_length: 2
    batch_size: 3
    memory_slot_learning_rate: 0.003
    learning_rate: 0.001
    rnn_dropout: 0.5
    adam_beta1: 0.9  # beta_1
    adam_beta2: 0.999  # beta_2
    ae_checkpoint_path: "/checkpoints/rat_autoencoder-epoch=402-train_loss=0.00.ckpt"
    hidden_size_RNN1: 32
    hidden_size_RNN2: 768
    hidden_size_RNN3: 128

logging:
    seed: 1234
    exp: test_traj
    n_traj: 800
